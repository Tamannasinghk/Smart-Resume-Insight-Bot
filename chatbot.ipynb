{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " # Install Required Libraries"
      ],
      "metadata": {
        "id": "GjHELFVaARk1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqv6ssR4AC4U",
        "outputId": "e3b7193b-0713-4a7d-ca6a-d99edb7d889b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-community pymupdf\n",
        "!pip install -q sentence-transformers faiss-cpu\n",
        "!pip install -q langchain-community google-generativeai\n",
        "!pip install --upgrade langchain langchain-community\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saved PDF File"
      ],
      "metadata": {
        "id": "Y-xQzxk6AmBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_uploaded_files_to_document(uploaded_files: List[str], save_dir: str = \"Document\"):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    saved_count = 0\n",
        "\n",
        "    for file in uploaded_files:\n",
        "        filename = os.path.basename(file.name if hasattr(file, \"name\") else file)\n",
        "        if filename.lower().endswith(\".pdf\"):\n",
        "            dest_path = os.path.join(save_dir, filename)\n",
        "            shutil.copy(file.name if hasattr(file, \"name\") else file, dest_path)\n",
        "            saved_count += 1\n",
        "\n",
        "    return saved_count\n"
      ],
      "metadata": {
        "id": "bkzGrRUsAldh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load PDFs as LangChain Documents"
      ],
      "metadata": {
        "id": "9vzMSXfCBFI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents_from_folder(folder_path: str = \"Document\"):\n",
        "    all_documents = []\n",
        "\n",
        "    for idx, filename in enumerate(os.listdir(folder_path)):\n",
        "        if filename.lower().endswith(\".pdf\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            loader = PyMuPDFLoader(file_path)\n",
        "            docs = loader.load()\n",
        "\n",
        "            # Added metadata\n",
        "            for doc in docs:\n",
        "                doc.metadata[\"doc_number\"] = f\"doc_{idx+1}\"\n",
        "                doc.metadata[\"filename\"] = filename\n",
        "\n",
        "            all_documents.extend(docs)\n",
        "\n",
        "    print(f\"Loaded {len(all_documents)} pages from {len(os.listdir(folder_path))} PDFs.\")\n",
        "    return all_documents\n"
      ],
      "metadata": {
        "id": "5J1Q6U2bA2Tw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_documents_and_save_to_csv(documents: List[Document], csv_path: str = \"parsed_documents.csv\"):\n",
        "    data_records = []\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"],\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=100\n",
        "    )\n",
        "\n",
        "    for doc in documents:\n",
        "        doc_id = doc.metadata.get(\"doc_number\", \"unknown_doc\")\n",
        "        page_no = doc.metadata.get(\"page\", -1)\n",
        "        text = doc.page_content.strip()\n",
        "        chunks = text_splitter.split_text(text)\n",
        "\n",
        "        for pehra_no, chunk in enumerate(chunks, start=1):\n",
        "            data_records.append({\n",
        "                \"doc_id\": doc_id,\n",
        "                \"page_no\": page_no,\n",
        "                \"pehra_no\": pehra_no,\n",
        "                \"text\": chunk\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(data_records)\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"CSV created with {len(df)} rows. Saved as: {csv_path}\")\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "WJNV5qsVA6b_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text embeddings, build FAISS index and save metadata\n",
        "def embed_and_index_documents(csv_path: str = \"parsed_documents.csv\",\n",
        "                              model_name: str = \"all-MiniLM-L6-v2\",\n",
        "                              index_path: str = \"docs_faiss.index\",\n",
        "                              metadata_path: str = \"docs_metadata.pkl\"):\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    model = SentenceTransformer(model_name)\n",
        "    embeddings = model.encode(df['text'].tolist(), show_progress_bar=True)\n",
        "    embeddings = np.array(embeddings).astype(\"float32\")\n",
        "\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(embeddings)\n",
        "\n",
        "    faiss.write_index(index, index_path)\n",
        "\n",
        "    metadata = df.to_dict(orient=\"records\")\n",
        "    with open(metadata_path, \"wb\") as f:\n",
        "        pickle.dump(metadata, f)\n",
        "\n",
        "    print(f\"Saved: FAISS ➝ `{index_path}`, Metadata ➝ `{metadata_path}`\")\n",
        "    return index, metadata\n"
      ],
      "metadata": {
        "id": "k1NNee0IBf0o"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "\n",
        "# Function to search top matching document chunks using FAISS\n",
        "def search_docs(query: str, top_k: int = 5):\n",
        "    if not query.strip():\n",
        "        return []\n",
        "\n",
        "    query_embedding = model.encode([query]).astype(\"float32\")\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    results = []\n",
        "    for idx, dist in zip(indices[0], distances[0]):\n",
        "        if idx < len(metadata):\n",
        "            similarity = 1 / (1 + dist)\n",
        "            results.append({\n",
        "                \"doc_id\": metadata[idx][\"doc_id\"],\n",
        "                \"page_no\": metadata[idx][\"page_no\"],\n",
        "                \"pehra_no\": metadata[idx][\"pehra_no\"],\n",
        "                \"text\": metadata[idx][\"text\"],\n",
        "                \"similarity_score\": round(similarity, 4)\n",
        "            })\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "zE_Bu9duBf3w"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt Template to form a structured QA input for LLM\n",
        "def generate_prompt_from_query(query: str, top_k: int = 5):\n",
        "    search_results = search_docs(query, top_k=top_k)\n",
        "\n",
        "    if not search_results:\n",
        "        return f\"Context:\\n\\n(No relevant documents found)\\n\\nQuestion:\\n{query}\\nAnswer:\\n\"\n",
        "\n",
        "    context_chunks = []\n",
        "    for r in search_results:\n",
        "        chunk = f\"{r['text']} (Source: doc_id: {r['doc_id']}, page: {r['page_no']}, pehra: {r['pehra_no']})\"\n",
        "        context_chunks.append(chunk)\n",
        "    context = \"\\n\\n\".join(context_chunks)\n",
        "\n",
        "    return qa_prompt.format(context=context, question=query)\n"
      ],
      "metadata": {
        "id": "mDykEk24Bf6o"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Gemini LLM to get the final answer from the prompt"
      ],
      "metadata": {
        "id": "rRHHeZ4qCtTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer_from_llm(prompt: str)\n",
        "    try:\n",
        "        chain = llm | parser\n",
        "        response = chain.invoke(prompt)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        return f\"Error generating response: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "OE2JlexICTRW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_themes_from_query(query: str, top_k: int = 5)\n",
        "    try:\n",
        "        retrieved_results = search_docs(query, top_k=top_k)\n",
        "        if not retrieved_results:\n",
        "            return \"No relevant context found for theme extraction.\"\n",
        "\n",
        "        context_chunks = [\n",
        "            f\"Text: {r['text']}\\nSource: doc_id: {r['doc_id']}, page: {r['page_no']}, pehra: {r['pehra_no']}\"\n",
        "            for r in retrieved_results\n",
        "        ]\n",
        "        context = \"\\n\\n\".join(context_chunks)\n",
        "        final_prompt = theme_prompt.format(context=context, question=query)\n",
        "\n",
        "        response = llm.invoke(final_prompt)\n",
        "        return response.content\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\" Error generating themes: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "VasK_RCsCU9t"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Gradio app for uploading documents and asking questions"
      ],
      "metadata": {
        "id": "9AfOjQHHCjAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def handle_upload(files: List[Path]):\n",
        "    document_dir = \"Document\"\n",
        "    if os.path.exists(document_dir):\n",
        "        shutil.rmtree(document_dir)\n",
        "    os.makedirs(document_dir, exist_ok=True)\n",
        "\n",
        "    for file in files:\n",
        "        dest_path = os.path.join(document_dir, os.path.basename(file.name))\n",
        "        if os.path.abspath(file) != os.path.abspath(dest_path):\n",
        "            shutil.copy(file, dest_path)\n",
        "\n",
        "    documents = load_documents_from_folder(document_dir)\n",
        "    split_documents_and_save_to_csv(documents)\n",
        "    embed_and_index_documents()\n",
        "\n",
        "    return f\" Uploaded {len(files)} document(s), old documents removed. Ready for queries!\"\n",
        "\n",
        "def handle_query(query):\n",
        "    if not query.strip():\n",
        "        return \"Please enter a query.\", \"\", \"\"\n",
        "\n",
        "    search_results = search_docs(query, top_k=5)\n",
        "    retrieved_texts = \"\\n\\n\".join([\n",
        "        f\"{r['text']}\\n(Source: doc_id: {r['doc_id']}, page: {r['page_no']}, pehra: {r['pehra_no']}, similarity: {r['similarity_score']})\"\n",
        "        for r in search_results\n",
        "    ])\n",
        "\n",
        "    prompt = generate_prompt_from_query(query, top_k=5)\n",
        "    answer = get_answer_from_llm(prompt)\n",
        "    themes = generate_themes_from_query(query, top_k=15)\n",
        "\n",
        "    return retrieved_texts, answer, themes\n"
      ],
      "metadata": {
        "id": "pQ8h16UtCWcV"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-oYd8kxCYQV"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}